{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "First 5 training samples (X_train and corresponding rewards):\n",
      "X_train[0] = [6 2 6 7 4], reward = 25\n",
      "X_train[1] = [3 7 2 5 4], reward = 21\n",
      "X_train[2] = [1 7 3 5 1], reward = 17\n",
      "X_train[3] = [7 4 0 5 8], reward = 24\n",
      "X_train[4] = [0 2 3 6 3], reward = 14\n",
      "\n",
      "First 5 testing samples (X_test and corresponding rewards):\n",
      "X_test[0] = [4 8 3 4 8], reward = 27\n",
      "X_test[1] = [7 2 0 2 3], reward = 14\n",
      "X_test[2] = [1 0 6 7 6], reward = 20\n",
      "X_test[3] = [5 4 3 0 6], reward = 18\n",
      "X_test[4] = [4 6 0 2 8], reward = 20\n"
     ]
    }
   ],
   "source": [
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Number of prizes\n",
    "n = 5\n",
    "\n",
    "# Create a random d vector for n prizes (upper bounds on collection)\n",
    "# For example, each d_i is drawn uniformly from 1 to 5\n",
    "d = np.random.randint(1, 6, size=n)\n",
    "\n",
    "# Define the reward function\n",
    "def reward_function(x, d):\n",
    "    # x: array of collected amounts (shape: (n,))\n",
    "    # d: array of upper bounds (shape: (n,))\n",
    "    return np.sum(np.minimum(x, d))\n",
    "\n",
    "def modular_reward_function(x, d):\n",
    "    return np.sum(x)\n",
    "\n",
    "# Function to generate a dataset\n",
    "def generate_dataset(num_samples, d, extra=3, modular=False):\n",
    "    # For each prize i, sample x_i uniformly from 0 to d[i] + extra\n",
    "    X = np.zeros((num_samples, len(d)), dtype=int)\n",
    "    y = np.zeros(num_samples, dtype=int)\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        sample = [np.random.randint(0, d_i + extra + 1) for d_i in d]\n",
    "        X[i, :] = sample\n",
    "        if modular:\n",
    "            y[i] = modular_reward_function(sample, d)\n",
    "        else:\n",
    "            y[i] = reward_function(sample, d)\n",
    "\n",
    "    return X, y\n",
    "\n",
    "# Generate training and testing sets\n",
    "num_train = 100\n",
    "num_test = 30\n",
    "\n",
    "X_train, y_train = generate_dataset(num_train, d, modular=True)\n",
    "X_test, y_test = generate_dataset(num_test, d, modular=True)\n",
    "\n",
    "print(\"\\nFirst 5 training samples (X_train and corresponding rewards):\")\n",
    "for i in range(5):\n",
    "    print(\"X_train[{}] = {}, reward = {}\".format(i, X_train[i], y_train[i]))\n",
    "\n",
    "print(\"\\nFirst 5 testing samples (X_test and corresponding rewards):\")\n",
    "for i in range(5):\n",
    "    print(\"X_test[{}] = {}, reward = {}\".format(i, X_test[i], y_test[i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5])\n",
      "x\n",
      "tensor([[3., 7., 2., 5., 4.]])\n",
      "outputs\n",
      "tensor([[0.]], grad_fn=<SumBackward1>)\n",
      "torch.Size([1, 5])\n",
      "x\n",
      "tensor([[0., 3., 0., 0., 3.]])\n",
      "outputs\n",
      "tensor([[0.]], grad_fn=<SumBackward1>)\n",
      "torch.Size([1, 5])\n",
      "x\n",
      "tensor([[3., 0., 1., 0., 4.]])\n",
      "outputs\n",
      "tensor([[0.]], grad_fn=<SumBackward1>)\n",
      "torch.Size([1, 5])\n",
      "x\n",
      "tensor([[0., 7., 3., 1., 0.]])\n",
      "outputs\n",
      "tensor([[0.]], grad_fn=<SumBackward1>)\n",
      "torch.Size([1, 5])\n",
      "x\n",
      "tensor([[7., 4., 0., 5., 8.]])\n",
      "outputs\n",
      "tensor([[0.]], grad_fn=<SumBackward1>)\n",
      "torch.Size([1, 5])\n",
      "x\n",
      "tensor([[2., 8., 0., 1., 6.]])\n",
      "outputs\n",
      "tensor([[0.]], grad_fn=<SumBackward1>)\n",
      "torch.Size([1, 5])\n",
      "x\n",
      "tensor([[1., 3., 5., 7., 6.]])\n",
      "outputs\n",
      "tensor([[0.]], grad_fn=<SumBackward1>)\n",
      "torch.Size([1, 5])\n",
      "x\n",
      "tensor([[7., 8., 5., 4., 0.]])\n",
      "outputs\n",
      "tensor([[0.]], grad_fn=<SumBackward1>)\n",
      "torch.Size([1, 5])\n",
      "x\n",
      "tensor([[6., 5., 5., 5., 5.]])\n",
      "outputs\n",
      "tensor([[0.]], grad_fn=<SumBackward1>)\n",
      "torch.Size([1, 5])\n",
      "x\n",
      "tensor([[2., 4., 0., 7., 4.]])\n",
      "outputs\n",
      "tensor([[0.]], grad_fn=<SumBackward1>)\n",
      "torch.Size([1, 5])\n",
      "x\n",
      "tensor([[4., 6., 6., 6., 8.]])\n",
      "outputs\n",
      "tensor([[0.]], grad_fn=<SumBackward1>)\n",
      "torch.Size([1, 5])\n",
      "x\n",
      "tensor([[7., 6., 6., 4., 5.]])\n",
      "outputs\n",
      "tensor([[0.]], grad_fn=<SumBackward1>)\n",
      "torch.Size([1, 5])\n",
      "x\n",
      "tensor([[6., 8., 3., 4., 0.]])\n",
      "outputs\n",
      "tensor([[0.]], grad_fn=<SumBackward1>)\n",
      "torch.Size([1, 5])\n",
      "x\n",
      "tensor([[6., 1., 1., 6., 5.]])\n",
      "outputs\n",
      "tensor([[0.]], grad_fn=<SumBackward1>)\n",
      "torch.Size([1, 5])\n",
      "x\n",
      "tensor([[4., 0., 0., 0., 1.]])\n",
      "outputs\n",
      "tensor([[0.]], grad_fn=<SumBackward1>)\n",
      "torch.Size([1, 5])\n",
      "x\n",
      "tensor([[0., 7., 4., 6., 8.]])\n",
      "outputs\n",
      "tensor([[0.]], grad_fn=<SumBackward1>)\n",
      "torch.Size([1, 5])\n",
      "x\n",
      "tensor([[5., 3., 5., 1., 1.]])\n",
      "outputs\n",
      "tensor([[0.]], grad_fn=<SumBackward1>)\n",
      "torch.Size([1, 5])\n",
      "x\n",
      "tensor([[3., 6., 1., 2., 0.]])\n",
      "outputs\n",
      "tensor([[0.]], grad_fn=<SumBackward1>)\n",
      "torch.Size([1, 5])\n",
      "x\n",
      "tensor([[3., 6., 3., 8., 0.]])\n",
      "outputs\n",
      "tensor([[0.]], grad_fn=<SumBackward1>)\n",
      "torch.Size([1, 5])\n",
      "x\n",
      "tensor([[7., 0., 3., 0., 7.]])\n",
      "outputs\n",
      "tensor([[0.]], grad_fn=<SumBackward1>)\n",
      "torch.Size([1, 5])\n",
      "x\n",
      "tensor([[3., 0., 5., 4., 6.]])\n",
      "outputs\n",
      "tensor([[0.]], grad_fn=<SumBackward1>)\n",
      "torch.Size([1, 5])\n",
      "x\n",
      "tensor([[7., 2., 2., 0., 4.]])\n",
      "outputs\n",
      "tensor([[0.]], grad_fn=<SumBackward1>)\n",
      "torch.Size([1, 5])\n",
      "x\n",
      "tensor([[1., 5., 4., 3., 1.]])\n",
      "outputs\n",
      "tensor([[0.]], grad_fn=<SumBackward1>)\n",
      "torch.Size([1, 5])\n",
      "x\n",
      "tensor([[1., 8., 3., 8., 0.]])\n",
      "outputs\n",
      "tensor([[0.]], grad_fn=<SumBackward1>)\n",
      "torch.Size([1, 5])\n",
      "x\n",
      "tensor([[4., 6., 2., 2., 4.]])\n",
      "outputs\n",
      "tensor([[0.]], grad_fn=<SumBackward1>)\n",
      "torch.Size([1, 5])\n",
      "x\n",
      "tensor([[0., 3., 0., 4., 3.]])\n",
      "outputs\n",
      "tensor([[0.]], grad_fn=<SumBackward1>)\n",
      "torch.Size([1, 5])\n",
      "x\n",
      "tensor([[3., 3., 2., 2., 2.]])\n",
      "outputs\n",
      "tensor([[0.]], grad_fn=<SumBackward1>)\n",
      "torch.Size([1, 5])\n",
      "x\n",
      "tensor([[1., 7., 3., 8., 4.]])\n",
      "outputs\n",
      "tensor([[0.]], grad_fn=<SumBackward1>)\n",
      "torch.Size([1, 5])\n",
      "x\n",
      "tensor([[2., 7., 6., 5., 7.]])\n",
      "outputs\n",
      "tensor([[0.]], grad_fn=<SumBackward1>)\n",
      "torch.Size([1, 5])\n",
      "x\n",
      "tensor([[1., 6., 1., 8., 6.]])\n",
      "outputs\n",
      "tensor([[0.]], grad_fn=<SumBackward1>)\n",
      "torch.Size([1, 5])\n",
      "x\n",
      "tensor([[3., 2., 3., 2., 0.]])\n",
      "outputs\n",
      "tensor([[0.]], grad_fn=<SumBackward1>)\n",
      "torch.Size([1, 5])\n",
      "x\n",
      "tensor([[7., 0., 3., 2., 5.]])\n",
      "outputs\n",
      "tensor([[0.]], grad_fn=<SumBackward1>)\n",
      "torch.Size([1, 5])\n",
      "x\n",
      "tensor([[4., 2., 5., 7., 1.]])\n",
      "outputs\n",
      "tensor([[0.]], grad_fn=<SumBackward1>)\n",
      "torch.Size([1, 5])\n",
      "x\n",
      "tensor([[4., 0., 3., 0., 4.]])\n",
      "outputs\n",
      "tensor([[0.]], grad_fn=<SumBackward1>)\n",
      "torch.Size([1, 5])\n",
      "x\n",
      "tensor([[3., 7., 6., 2., 0.]])\n",
      "outputs\n",
      "tensor([[0.]], grad_fn=<SumBackward1>)\n",
      "torch.Size([1, 5])\n",
      "x\n",
      "tensor([[7., 3., 4., 1., 2.]])\n",
      "outputs\n",
      "tensor([[0.]], grad_fn=<SumBackward1>)\n",
      "torch.Size([1, 5])\n",
      "x\n",
      "tensor([[0., 7., 6., 2., 6.]])\n",
      "outputs\n",
      "tensor([[0.]], grad_fn=<SumBackward1>)\n",
      "torch.Size([1, 5])\n",
      "x\n",
      "tensor([[3., 1., 3., 1., 5.]])\n",
      "outputs\n",
      "tensor([[0.]], grad_fn=<SumBackward1>)\n",
      "torch.Size([1, 5])\n",
      "x\n",
      "tensor([[3., 3., 5., 4., 6.]])\n",
      "outputs\n",
      "tensor([[0.]], grad_fn=<SumBackward1>)\n",
      "torch.Size([1, 5])\n",
      "x\n",
      "tensor([[4., 6., 0., 8., 2.]])\n",
      "outputs\n",
      "tensor([[0.]], grad_fn=<SumBackward1>)\n",
      "torch.Size([1, 5])\n",
      "x\n",
      "tensor([[1., 5., 2., 8., 3.]])\n",
      "outputs\n",
      "tensor([[0.]], grad_fn=<SumBackward1>)\n",
      "torch.Size([1, 5])\n",
      "x\n",
      "tensor([[3., 8., 2., 6., 5.]])\n",
      "outputs\n",
      "tensor([[0.]], grad_fn=<SumBackward1>)\n",
      "torch.Size([1, 5])\n",
      "x\n",
      "tensor([[3., 5., 6., 3., 6.]])\n",
      "outputs\n",
      "tensor([[0.]], grad_fn=<SumBackward1>)\n",
      "torch.Size([1, 5])\n",
      "x\n",
      "tensor([[7., 1., 6., 4., 7.]])\n",
      "outputs\n",
      "tensor([[0.]], grad_fn=<SumBackward1>)\n",
      "torch.Size([1, 5])\n",
      "x\n",
      "tensor([[4., 6., 1., 2., 1.]])\n",
      "outputs\n",
      "tensor([[0.]], grad_fn=<SumBackward1>)\n",
      "torch.Size([1, 5])\n",
      "x\n",
      "tensor([[3., 3., 0., 7., 2.]])\n",
      "outputs\n",
      "tensor([[0.]], grad_fn=<SumBackward1>)\n",
      "torch.Size([1, 5])\n",
      "x\n",
      "tensor([[7., 8., 4., 4., 0.]])\n",
      "outputs\n",
      "tensor([[0.]], grad_fn=<SumBackward1>)\n",
      "torch.Size([1, 5])\n",
      "x\n",
      "tensor([[3., 7., 6., 2., 0.]])\n",
      "outputs\n",
      "tensor([[0.]], grad_fn=<SumBackward1>)\n",
      "torch.Size([1, 5])\n",
      "x\n",
      "tensor([[5., 1., 1., 8., 4.]])\n",
      "outputs\n",
      "tensor([[0.]], grad_fn=<SumBackward1>)\n",
      "torch.Size([1, 5])\n",
      "x\n",
      "tensor([[6., 3., 4., 6., 2.]])\n",
      "outputs\n",
      "tensor([[0.]], grad_fn=<SumBackward1>)\n",
      "torch.Size([1, 5])\n",
      "x\n",
      "tensor([[2., 6., 0., 7., 1.]])\n",
      "outputs\n",
      "tensor([[0.]], grad_fn=<SumBackward1>)\n",
      "torch.Size([1, 5])\n",
      "x\n",
      "tensor([[4., 3., 5., 2., 6.]])\n",
      "outputs\n",
      "tensor([[0.]], grad_fn=<SumBackward1>)\n",
      "torch.Size([1, 5])\n",
      "x\n",
      "tensor([[0., 8., 2., 5., 6.]])\n",
      "outputs\n",
      "tensor([[0.]], grad_fn=<SumBackward1>)\n",
      "torch.Size([1, 5])\n",
      "x\n",
      "tensor([[4., 3., 6., 1., 0.]])\n",
      "outputs\n",
      "tensor([[0.]], grad_fn=<SumBackward1>)\n",
      "torch.Size([1, 5])\n",
      "x\n",
      "tensor([[2., 8., 4., 7., 0.]])\n",
      "outputs\n",
      "tensor([[0.]], grad_fn=<SumBackward1>)\n",
      "torch.Size([1, 5])\n",
      "x\n",
      "tensor([[0., 2., 3., 6., 3.]])\n",
      "outputs\n",
      "tensor([[0.]], grad_fn=<SumBackward1>)\n",
      "torch.Size([1, 5])\n",
      "x\n",
      "tensor([[2., 8., 1., 5., 5.]])\n",
      "outputs\n",
      "tensor([[0.]], grad_fn=<SumBackward1>)\n",
      "torch.Size([1, 5])\n",
      "x\n",
      "tensor([[6., 2., 6., 6., 0.]])\n",
      "outputs\n",
      "tensor([[0.]], grad_fn=<SumBackward1>)\n",
      "torch.Size([1, 5])\n",
      "x\n",
      "tensor([[5., 4., 6., 2., 0.]])\n",
      "outputs\n",
      "tensor([[0.]], grad_fn=<SumBackward1>)\n",
      "torch.Size([1, 5])\n",
      "x\n",
      "tensor([[3., 5., 3., 2., 8.]])\n",
      "outputs\n",
      "tensor([[0.]], grad_fn=<SumBackward1>)\n",
      "torch.Size([1, 5])\n",
      "x\n",
      "tensor([[6., 2., 6., 7., 4.]])\n",
      "outputs\n",
      "tensor([[0.]], grad_fn=<SumBackward1>)\n",
      "torch.Size([1, 5])\n",
      "x\n",
      "tensor([[1., 4., 1., 4., 6.]])\n",
      "outputs\n",
      "tensor([[0.]], grad_fn=<SumBackward1>)\n",
      "torch.Size([1, 5])\n",
      "x\n",
      "tensor([[7., 6., 6., 7., 4.]])\n",
      "outputs\n",
      "tensor([[0.]], grad_fn=<SumBackward1>)\n",
      "torch.Size([1, 5])\n",
      "x\n",
      "tensor([[6., 4., 0., 6., 1.]])\n",
      "outputs\n",
      "tensor([[0.]], grad_fn=<SumBackward1>)\n",
      "torch.Size([1, 5])\n",
      "x\n",
      "tensor([[3., 2., 0., 0., 7.]])\n",
      "outputs\n",
      "tensor([[0.]], grad_fn=<SumBackward1>)\n",
      "torch.Size([1, 5])\n",
      "x\n",
      "tensor([[3., 4., 6., 3., 5.]])\n",
      "outputs\n",
      "tensor([[0.]], grad_fn=<SumBackward1>)\n",
      "torch.Size([1, 5])\n",
      "x\n",
      "tensor([[1., 1., 1., 0., 7.]])\n",
      "outputs\n",
      "tensor([[0.]], grad_fn=<SumBackward1>)\n",
      "torch.Size([1, 5])\n",
      "x\n",
      "tensor([[5., 8., 4., 4., 0.]])\n",
      "outputs\n",
      "tensor([[0.]], grad_fn=<SumBackward1>)\n",
      "torch.Size([1, 5])\n",
      "x\n",
      "tensor([[7., 7., 6., 2., 0.]])\n",
      "outputs\n",
      "tensor([[0.]], grad_fn=<SumBackward1>)\n",
      "torch.Size([1, 5])\n",
      "x\n",
      "tensor([[7., 7., 1., 5., 6.]])\n",
      "outputs\n",
      "tensor([[0.]], grad_fn=<SumBackward1>)\n",
      "torch.Size([1, 5])\n",
      "x\n",
      "tensor([[6., 0., 0., 8., 8.]])\n",
      "outputs\n",
      "tensor([[0.]], grad_fn=<SumBackward1>)\n",
      "torch.Size([1, 5])\n",
      "x\n",
      "tensor([[2., 8., 6., 3., 4.]])\n",
      "outputs\n",
      "tensor([[0.]], grad_fn=<SumBackward1>)\n",
      "torch.Size([1, 5])\n",
      "x\n",
      "tensor([[3., 3., 3., 1., 2.]])\n",
      "outputs\n",
      "tensor([[0.]], grad_fn=<SumBackward1>)\n",
      "torch.Size([1, 5])\n",
      "x\n",
      "tensor([[3., 2., 1., 0., 7.]])\n",
      "outputs\n",
      "tensor([[0.]], grad_fn=<SumBackward1>)\n",
      "torch.Size([1, 5])\n",
      "x\n",
      "tensor([[7., 4., 3., 1., 5.]])\n",
      "outputs\n",
      "tensor([[0.]], grad_fn=<SumBackward1>)\n",
      "torch.Size([1, 5])\n",
      "x\n",
      "tensor([[0., 2., 5., 1., 4.]])\n",
      "outputs\n",
      "tensor([[0.]], grad_fn=<SumBackward1>)\n",
      "torch.Size([1, 5])\n",
      "x\n",
      "tensor([[2., 2., 3., 7., 5.]])\n",
      "outputs\n",
      "tensor([[0.]], grad_fn=<SumBackward1>)\n",
      "torch.Size([1, 5])\n",
      "x\n",
      "tensor([[6., 2., 6., 8., 3.]])\n",
      "outputs\n",
      "tensor([[0.]], grad_fn=<SumBackward1>)\n",
      "torch.Size([1, 5])\n",
      "x\n",
      "tensor([[7., 0., 3., 4., 5.]])\n",
      "outputs\n",
      "tensor([[0.]], grad_fn=<SumBackward1>)\n",
      "torch.Size([1, 5])\n",
      "x\n",
      "tensor([[2., 8., 4., 1., 1.]])\n",
      "outputs\n",
      "tensor([[0.]], grad_fn=<SumBackward1>)\n",
      "torch.Size([1, 5])\n",
      "x\n",
      "tensor([[0., 3., 1., 5., 5.]])\n",
      "outputs\n",
      "tensor([[0.]], grad_fn=<SumBackward1>)\n",
      "torch.Size([1, 5])\n",
      "x\n",
      "tensor([[1., 1., 4., 2., 1.]])\n",
      "outputs\n",
      "tensor([[0.]], grad_fn=<SumBackward1>)\n",
      "torch.Size([1, 5])\n",
      "x\n",
      "tensor([[3., 8., 5., 7., 4.]])\n",
      "outputs\n",
      "tensor([[0.]], grad_fn=<SumBackward1>)\n",
      "torch.Size([1, 5])\n",
      "x\n",
      "tensor([[2., 7., 5., 2., 0.]])\n",
      "outputs\n",
      "tensor([[0.]], grad_fn=<SumBackward1>)\n",
      "torch.Size([1, 5])\n",
      "x\n",
      "tensor([[2., 4., 6., 2., 0.]])\n",
      "outputs\n",
      "tensor([[0.]], grad_fn=<SumBackward1>)\n",
      "torch.Size([1, 5])\n",
      "x\n",
      "tensor([[1., 0., 6., 1., 5.]])\n",
      "outputs\n",
      "tensor([[0.]], grad_fn=<SumBackward1>)\n",
      "torch.Size([1, 5])\n",
      "x\n",
      "tensor([[7., 1., 0., 5., 2.]])\n",
      "outputs\n",
      "tensor([[0.]], grad_fn=<SumBackward1>)\n",
      "torch.Size([1, 5])\n",
      "x\n",
      "tensor([[3., 4., 6., 0., 2.]])\n",
      "outputs\n",
      "tensor([[0.]], grad_fn=<SumBackward1>)\n",
      "torch.Size([1, 5])\n",
      "x\n",
      "tensor([[0., 6., 0., 7., 0.]])\n",
      "outputs\n",
      "tensor([[0.]], grad_fn=<SumBackward1>)\n",
      "torch.Size([1, 5])\n",
      "x\n",
      "tensor([[1., 4., 1., 3., 6.]])\n",
      "outputs\n",
      "tensor([[0.]], grad_fn=<SumBackward1>)\n",
      "torch.Size([1, 5])\n",
      "x\n",
      "tensor([[1., 7., 3., 5., 1.]])\n",
      "outputs\n",
      "tensor([[0.]], grad_fn=<SumBackward1>)\n",
      "torch.Size([1, 5])\n",
      "x\n",
      "tensor([[7., 6., 1., 7., 0.]])\n",
      "outputs\n",
      "tensor([[0.]], grad_fn=<SumBackward1>)\n",
      "torch.Size([1, 5])\n",
      "x\n",
      "tensor([[5., 3., 5., 6., 8.]])\n",
      "outputs\n",
      "tensor([[0.]], grad_fn=<SumBackward1>)\n",
      "torch.Size([1, 5])\n",
      "x\n",
      "tensor([[1., 5., 6., 4., 0.]])\n",
      "outputs\n",
      "tensor([[0.]], grad_fn=<SumBackward1>)\n",
      "torch.Size([1, 5])\n",
      "x\n",
      "tensor([[7., 8., 2., 4., 2.]])\n",
      "outputs\n",
      "tensor([[0.]], grad_fn=<SumBackward1>)\n",
      "torch.Size([1, 5])\n",
      "x\n",
      "tensor([[3., 0., 0., 2., 4.]])\n",
      "outputs\n",
      "tensor([[0.]], grad_fn=<SumBackward1>)\n",
      "torch.Size([1, 5])\n",
      "x\n",
      "tensor([[3., 8., 3., 1., 8.]])\n",
      "outputs\n",
      "tensor([[0.]], grad_fn=<SumBackward1>)\n",
      "torch.Size([1, 5])\n",
      "x\n",
      "tensor([[5., 0., 0., 5., 2.]])\n",
      "outputs\n",
      "tensor([[0.]], grad_fn=<SumBackward1>)\n",
      "torch.Size([1, 5])\n",
      "x\n",
      "tensor([[5., 3., 4., 4., 6.]])\n",
      "outputs\n",
      "tensor([[0.]], grad_fn=<SumBackward1>)\n",
      "torch.Size([1, 5])\n",
      "x\n",
      "tensor([[2., 7., 2., 0., 5.]])\n",
      "outputs\n",
      "tensor([[0.]], grad_fn=<SumBackward1>)\n"
     ]
    }
   ],
   "source": [
    "from dqn import MonotoneSubmodularNet\n",
    "# Hyperparameters\n",
    "learning_rate = 1e-2\n",
    "num_epochs = 100\n",
    "batch_size = 1\n",
    "\n",
    "model = MonotoneSubmodularNet([1, 10, 1], 0.5, 1, 1)\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "for batch_X, batch_y in train_loader:\n",
    "    outputs = model(batch_X)\n",
    "    print(batch_X.shape)\n",
    "    print(\"x\")\n",
    "    print(batch_X)\n",
    "    print(\"outputs\")\n",
    "    print(outputs)\n",
    "\n",
    "    # print(outputs)\n",
    "    # print(\"true\")\n",
    "    # print(batch_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100 - Train Loss: 358.4200, Test Loss: 409.9000\n",
      "Epoch 2/100 - Train Loss: 358.4200, Test Loss: 409.9000\n",
      "Epoch 3/100 - Train Loss: 358.4200, Test Loss: 409.9000\n",
      "Epoch 4/100 - Train Loss: 358.4200, Test Loss: 409.9000\n",
      "Epoch 5/100 - Train Loss: 358.4200, Test Loss: 409.9000\n",
      "Epoch 6/100 - Train Loss: 358.4200, Test Loss: 409.9000\n",
      "Epoch 7/100 - Train Loss: 358.4200, Test Loss: 409.9000\n",
      "Epoch 8/100 - Train Loss: 358.4200, Test Loss: 409.9000\n",
      "Epoch 9/100 - Train Loss: 358.4200, Test Loss: 409.9000\n",
      "Epoch 10/100 - Train Loss: 358.4200, Test Loss: 409.9000\n",
      "Epoch 11/100 - Train Loss: 358.4200, Test Loss: 409.9000\n",
      "Epoch 12/100 - Train Loss: 358.4200, Test Loss: 409.9000\n",
      "Epoch 13/100 - Train Loss: 358.4200, Test Loss: 409.9000\n",
      "Epoch 14/100 - Train Loss: 358.4200, Test Loss: 409.9000\n",
      "Epoch 15/100 - Train Loss: 358.4200, Test Loss: 409.9000\n",
      "Epoch 16/100 - Train Loss: 358.4200, Test Loss: 409.9000\n",
      "Epoch 17/100 - Train Loss: 358.4200, Test Loss: 409.9000\n",
      "Epoch 18/100 - Train Loss: 358.4200, Test Loss: 409.9000\n",
      "Epoch 19/100 - Train Loss: 358.4200, Test Loss: 409.9000\n",
      "Epoch 20/100 - Train Loss: 358.4200, Test Loss: 409.9000\n",
      "Epoch 21/100 - Train Loss: 358.4200, Test Loss: 409.9000\n",
      "Epoch 22/100 - Train Loss: 358.4200, Test Loss: 409.9000\n",
      "Epoch 23/100 - Train Loss: 358.4200, Test Loss: 409.9000\n",
      "Epoch 24/100 - Train Loss: 358.4200, Test Loss: 409.9000\n",
      "Epoch 25/100 - Train Loss: 358.4200, Test Loss: 409.9000\n",
      "Epoch 26/100 - Train Loss: 358.4200, Test Loss: 409.9000\n",
      "Epoch 27/100 - Train Loss: 358.4200, Test Loss: 409.9000\n",
      "Epoch 28/100 - Train Loss: 358.4200, Test Loss: 409.9000\n",
      "Epoch 29/100 - Train Loss: 358.4200, Test Loss: 409.9000\n",
      "Epoch 30/100 - Train Loss: 358.4200, Test Loss: 409.9000\n",
      "Epoch 31/100 - Train Loss: 358.4200, Test Loss: 409.9000\n",
      "Epoch 32/100 - Train Loss: 358.4200, Test Loss: 409.9000\n",
      "Epoch 33/100 - Train Loss: 358.4200, Test Loss: 409.9000\n",
      "Epoch 34/100 - Train Loss: 358.4200, Test Loss: 409.9000\n",
      "Epoch 35/100 - Train Loss: 358.4200, Test Loss: 409.9000\n",
      "Epoch 36/100 - Train Loss: 358.4200, Test Loss: 409.9000\n",
      "Epoch 37/100 - Train Loss: 358.4200, Test Loss: 409.9000\n",
      "Epoch 38/100 - Train Loss: 358.4200, Test Loss: 409.9000\n",
      "Epoch 39/100 - Train Loss: 358.4200, Test Loss: 409.9000\n",
      "Epoch 40/100 - Train Loss: 358.4200, Test Loss: 409.9000\n",
      "Epoch 41/100 - Train Loss: 358.4200, Test Loss: 409.9000\n",
      "Epoch 42/100 - Train Loss: 358.4200, Test Loss: 409.9000\n",
      "Epoch 43/100 - Train Loss: 358.4200, Test Loss: 409.9000\n",
      "Epoch 44/100 - Train Loss: 358.4200, Test Loss: 409.9000\n",
      "Epoch 45/100 - Train Loss: 358.4200, Test Loss: 409.9000\n",
      "Epoch 46/100 - Train Loss: 358.4200, Test Loss: 409.9000\n",
      "Epoch 47/100 - Train Loss: 358.4200, Test Loss: 409.9000\n",
      "Epoch 48/100 - Train Loss: 358.4200, Test Loss: 409.9000\n",
      "Epoch 49/100 - Train Loss: 358.4200, Test Loss: 409.9000\n",
      "Epoch 50/100 - Train Loss: 358.4200, Test Loss: 409.9000\n",
      "Epoch 51/100 - Train Loss: 358.4200, Test Loss: 409.9000\n",
      "Epoch 52/100 - Train Loss: 358.4200, Test Loss: 409.9000\n",
      "Epoch 53/100 - Train Loss: 358.4200, Test Loss: 409.9000\n",
      "Epoch 54/100 - Train Loss: 358.4200, Test Loss: 409.9000\n",
      "Epoch 55/100 - Train Loss: 358.4200, Test Loss: 409.9000\n",
      "Epoch 56/100 - Train Loss: 358.4200, Test Loss: 409.9000\n",
      "Epoch 57/100 - Train Loss: 358.4200, Test Loss: 409.9000\n",
      "Epoch 58/100 - Train Loss: 358.4200, Test Loss: 409.9000\n",
      "Epoch 59/100 - Train Loss: 358.4200, Test Loss: 409.9000\n",
      "Epoch 60/100 - Train Loss: 358.4200, Test Loss: 409.9000\n",
      "Epoch 61/100 - Train Loss: 358.4200, Test Loss: 409.9000\n",
      "Epoch 62/100 - Train Loss: 358.4200, Test Loss: 409.9000\n",
      "Epoch 63/100 - Train Loss: 358.4200, Test Loss: 409.9000\n",
      "Epoch 64/100 - Train Loss: 358.4200, Test Loss: 409.9000\n",
      "Epoch 65/100 - Train Loss: 358.4200, Test Loss: 409.9000\n",
      "Epoch 66/100 - Train Loss: 358.4200, Test Loss: 409.9000\n",
      "Epoch 67/100 - Train Loss: 358.4200, Test Loss: 409.9000\n",
      "Epoch 68/100 - Train Loss: 358.4200, Test Loss: 409.9000\n",
      "Epoch 69/100 - Train Loss: 358.4200, Test Loss: 409.9000\n",
      "Epoch 70/100 - Train Loss: 358.4200, Test Loss: 409.9000\n",
      "Epoch 71/100 - Train Loss: 358.4200, Test Loss: 409.9000\n",
      "Epoch 72/100 - Train Loss: 358.4200, Test Loss: 409.9000\n",
      "Epoch 73/100 - Train Loss: 358.4200, Test Loss: 409.9000\n",
      "Epoch 74/100 - Train Loss: 358.4200, Test Loss: 409.9000\n",
      "Epoch 75/100 - Train Loss: 358.4200, Test Loss: 409.9000\n",
      "Epoch 76/100 - Train Loss: 358.4200, Test Loss: 409.9000\n",
      "Epoch 77/100 - Train Loss: 358.4200, Test Loss: 409.9000\n",
      "Epoch 78/100 - Train Loss: 358.4200, Test Loss: 409.9000\n",
      "Epoch 79/100 - Train Loss: 358.4200, Test Loss: 409.9000\n",
      "Epoch 80/100 - Train Loss: 358.4200, Test Loss: 409.9000\n",
      "Epoch 81/100 - Train Loss: 358.4200, Test Loss: 409.9000\n",
      "Epoch 82/100 - Train Loss: 358.4200, Test Loss: 409.9000\n",
      "Epoch 83/100 - Train Loss: 358.4200, Test Loss: 409.9000\n",
      "Epoch 84/100 - Train Loss: 358.4200, Test Loss: 409.9000\n",
      "Epoch 85/100 - Train Loss: 358.4200, Test Loss: 409.9000\n",
      "Epoch 86/100 - Train Loss: 358.4200, Test Loss: 409.9000\n",
      "Epoch 87/100 - Train Loss: 358.4200, Test Loss: 409.9000\n",
      "Epoch 88/100 - Train Loss: 358.4200, Test Loss: 409.9000\n",
      "Epoch 89/100 - Train Loss: 358.4200, Test Loss: 409.9000\n",
      "Epoch 90/100 - Train Loss: 358.4200, Test Loss: 409.9000\n",
      "Epoch 91/100 - Train Loss: 358.4200, Test Loss: 409.9000\n",
      "Epoch 92/100 - Train Loss: 358.4200, Test Loss: 409.9000\n",
      "Epoch 93/100 - Train Loss: 358.4200, Test Loss: 409.9000\n",
      "Epoch 94/100 - Train Loss: 358.4200, Test Loss: 409.9000\n",
      "Epoch 95/100 - Train Loss: 358.4200, Test Loss: 409.9000\n",
      "Epoch 96/100 - Train Loss: 358.4200, Test Loss: 409.9000\n",
      "Epoch 97/100 - Train Loss: 358.4200, Test Loss: 409.9000\n",
      "Epoch 98/100 - Train Loss: 358.4200, Test Loss: 409.9000\n",
      "Epoch 99/100 - Train Loss: 358.4200, Test Loss: 409.9000\n",
      "Epoch 100/100 - Train Loss: 358.4200, Test Loss: 409.9000\n"
     ]
    }
   ],
   "source": [
    "writer = SummaryWriter(log_dir='runs/submodular_net')\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # if epoch == 0:\n",
    "        #     for name, param in model.named_parameters():\n",
    "        #         if param.requires_grad and param.grad is not None and param.grad.sum() > 0:\n",
    "        #             print(f\"Parameter: {name}, Gradient: {param.grad}\")\n",
    "        #             print(f\"Parameter val: {param}\")\n",
    "\n",
    "        running_loss += loss.item() * batch_X.size(0)\n",
    "        model.clamp_weights()\n",
    "    \n",
    "    epoch_loss = running_loss / len(train_dataset)\n",
    "\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            writer.add_histogram(f'weights_{name}', param, epoch)\n",
    "        if param.grad is not None:\n",
    "            writer.add_histogram(f'gradients_{name}', param.grad, epoch)\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch_X, batch_y in test_loader:\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            test_loss += loss.item() * batch_X.size(0)\n",
    "    test_loss = test_loss / len(test_dataset)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - Train Loss: {epoch_loss:.4f}, Test Loss: {test_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    for batch_X, batch_y in test_loader:\n",
    "        outputs = model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        print(batch_X)\n",
    "        print(outputs, batch_y)\n",
    "        test_loss += loss.item() * batch_X.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_input = torch.tensor([[100, 100, 0, 0, 0]], dtype=torch.float32)\n",
    "model(example_input)\n",
    "print(model.m[0].weight.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dqn import IncreasingConcaveNet\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "writer = SummaryWriter(log_dir='runs/increasing_concave_net')\n",
    "\n",
    "# Define a known increasing concave function\n",
    "def true_function(x):\n",
    "    return torch.log(x)  # slight shift to avoid sqrt(0)\n",
    "\n",
    "# Generate training data\n",
    "x_train = torch.linspace(0.1, 10, 100).unsqueeze(1)  # shape (100, 1)\n",
    "y_train = true_function(x_train)\n",
    "\n",
    "# Instantiate the model\n",
    "model = IncreasingConcaveNet([1, 32, 32, 32, 1], 1.0)\n",
    "model.clamp_weights()  # clamp initial weights\n",
    "\n",
    "# Optimizer and loss\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(1000):\n",
    "    optimizer.zero_grad()\n",
    "    y_pred = model(x_train)\n",
    "    loss = loss_fn(y_pred, y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    model.clamp_weights()\n",
    "\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            writer.add_histogram(f'{name}_weights', param, epoch)\n",
    "        if param.grad is not None:\n",
    "            writer.add_histogram(f'gradients_{name}', param.grad, epoch)\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.item():.6f}\")\n",
    "    \n",
    "\n",
    "# Plot the result\n",
    "with torch.no_grad():\n",
    "    y_pred = model(x_train)\n",
    "    plt.plot(x_train.numpy(), y_train.numpy(), label='True function')\n",
    "    plt.plot(x_train.numpy(), y_pred.numpy(), label='Model output')\n",
    "    plt.legend()\n",
    "    plt.title(\"Fitting an Increasing Concave Function\")\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(\"f(x)\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dqn import IncreasingConcaveNet\n",
    "num_epochs = 50\n",
    "learning_rate = 1e-2\n",
    "model = IncreasingConcaveNet([5, 32, 32, 32, 1], 1.0)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.MSELoss()\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * batch_X.size(0)\n",
    "        # model.clamp_weights()\n",
    "    \n",
    "    epoch_loss = running_loss / len(train_dataset)\n",
    "\n",
    "    # for name, param in model.named_parameters():\n",
    "    #     if param.grad is not None:\n",
    "    #         print(param.grad)\n",
    "\n",
    "    # Evaluate on test set\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch_X, batch_y in test_loader:\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            test_loss += loss.item() * batch_X.size(0)\n",
    "    test_loss = test_loss / len(test_dataset)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - Train Loss: {epoch_loss:.4f}, Test Loss: {test_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    for batch_X, batch_y in test_loader:\n",
    "        outputs = model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        print(batch_X)\n",
    "        print(outputs, batch_y)\n",
    "        test_loss += loss.item() * batch_X.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 4 5]\n",
      "[[3 3 6]\n",
      " [2 2 5]\n",
      " [3 5 3]\n",
      " [3 2 1]\n",
      " [5 5 0]\n",
      " [3 5 7]\n",
      " [5 2 2]\n",
      " [5 2 6]\n",
      " [5 6 7]\n",
      " [1 2 4]]\n",
      "[10  9  9  5  6 11  6  9 11  7]\n"
     ]
    }
   ],
   "source": [
    "def generate_dataset(num_samples, d, extra=3, modular=False):\n",
    "    # For each prize i, sample x_i uniformly from 0 to d[i] + extra\n",
    "    X = np.zeros((num_samples, len(d)), dtype=int)\n",
    "    y = np.zeros(num_samples, dtype=int)\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        sample = [np.random.randint(0, d_i + extra + 1) for d_i in d]\n",
    "        X[i, :] = sample\n",
    "        if modular:\n",
    "            y[i] = modular_reward_function(sample, d)\n",
    "        else:\n",
    "            y[i] = reward_function(sample, d)\n",
    "\n",
    "    return X, y\n",
    "\n",
    "# Define the reward function\n",
    "def reward_function(x, d):\n",
    "    # x: array of collected amounts (shape: (n,))\n",
    "    # d: array of upper bounds (shape: (n,))\n",
    "    # print(x)    \n",
    "    # print(d)\n",
    "    # print(np.minimum(x, d))\n",
    "    return np.sum(np.minimum(x, d))\n",
    "\n",
    "def modular_reward_function(x, d):\n",
    "    return np.sum(x)\n",
    "\n",
    "d = np.random.randint(1, 6, size=3)\n",
    "X, y = generate_dataset(10, d, modular=False)\n",
    "print(d)\n",
    "print(X)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "submodular_nn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
