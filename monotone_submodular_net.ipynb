{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "10c07855-aa8d-45fb-ade2-edb7e935997c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "22e9bbf3-5dc6-4dbc-986b-819665146b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from submodular_monotone_net import SubmodularMonotoneNet\n",
    "import torch\n",
    "from tqdm import trange\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bd790256-0010-4a7c-ac75-a5fab1292ac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/devarora/ml/submodular_nn/increasing_convex_net.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(\n",
      "/Users/devarora/ml/submodular_nn/increasing_convex_net.py:25: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(\n"
     ]
    }
   ],
   "source": [
    "monotone_network = SubmodularMonotoneNet(5, 0.5, 1, [1, 100, 100, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5eda7b0c-a273-44ad-a158-5e9e5c538071",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SubmodularMonotoneNet(\n",
       "  (phi): IncreasingConvexNet(\n",
       "    (Ws): ParameterList(\n",
       "        (0): Parameter containing: [torch.float32 of size 100x1]\n",
       "        (1): Parameter containing: [torch.float32 of size 100x100]\n",
       "        (2): Parameter containing: [torch.float32 of size 1x100]\n",
       "    )\n",
       "    (bs): ParameterList(\n",
       "        (0): Parameter containing: [torch.float32 of size 100]\n",
       "        (1): Parameter containing: [torch.float32 of size 100]\n",
       "        (2): Parameter containing: [torch.float32 of size 1]\n",
       "    )\n",
       "  )\n",
       "  (m): ModuleList(\n",
       "    (0-4): 5 x Linear(in_features=1, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "monotone_network.clamp_weights()\n",
    "# monotone_network.activ = lambda x: torch.min(torch.zeros_like(x), x)\n",
    "\n",
    "monotone_network\n",
    "\n",
    "# with torch.no_grad():\n",
    "    # for layer in monotone_network.phi.bs:\n",
    "      #   layer.data = torch.clamp(layer.data, 0, torch.inf)\n",
    "    \n",
    "    # for i in range(monotone_network.layers):\n",
    "    #     monotone_network.m[i].bias.clamp_(0, torch.inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f4bf3965-373e-4a0f-b7d3-69cd242d9df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TParams:\n",
    "    def __init__(self, lr=0.001, b1=None, b2=None, iters=int(1e3)):\n",
    "        self.lr = lr\n",
    "        self.b1 = b1\n",
    "        self.b2 = b2\n",
    "        self.iters = iters\n",
    "\n",
    "    def __str__(self):\n",
    "        return json.dumps(\n",
    "            {\"lr\": self.lr, \"b1\": self.b1, \"b2\": self.b2, \"iters\": self.iters}\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f6ebe3e9-869a-4fef-baf4-5d7de0dc7ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(net, x, y):\n",
    "    return torch.sum((net.forward(x) - y) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5138cd55-043d-4829-b227-3fb626fc13e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, fit_fn, a, b, dx, tparams, verbose=True):\n",
    "    \"\"\"\n",
    "    Train the net to minimize a least squares objective between itself and the given fit_fn on the interval [a, b]\n",
    "    by fitting regularly sampled points [a, a+dx, a+2dx, ..., b].\n",
    "\n",
    "    fit_fn - (np.ndarray) -> (np.ndarray): map an input tensor to an output tensor\n",
    "    a - (torch.float32): beginning of interval\n",
    "    b - (torch.float32): end of interval\n",
    "    dx - (torch.float32): step size\n",
    "    tparams - TParams: parameters to use for training\n",
    "    \"\"\"\n",
    "    x = torch.tensor(\n",
    "        torch.arange(a, b, dx), dtype=torch.float32\n",
    "    ).view([-1, 1])\n",
    "    y = fit_fn(x)\n",
    "\n",
    "    opt = torch.optim.Adam(\n",
    "        params=net.parameters(), lr=tparams.lr\n",
    "    )\n",
    "\n",
    "    if verbose:\n",
    "        rng = trange(tparams.iters)\n",
    "    else:\n",
    "        rng = range(tparams.iters)\n",
    "\n",
    "    for itr in rng:\n",
    "        opt.zero_grad()\n",
    "        loss_this_itr = loss(net, x, y)\n",
    "        \n",
    "        loss_this_itr.backward()\n",
    "        if (itr % 100 == 0):\n",
    "            if (not verbose): print(f\"Iteration: {itr + 1}\")\n",
    "            print(loss_this_itr)\n",
    "        opt.step()\n",
    "        net.clamp_weights()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "090e7eda-d234-4d4e-a853-f6e245159fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(net, fit_fn, a, b, dx):\n",
    "    x = torch.arange(a, b, dx)\n",
    "    x_pt = torch.tensor(x, dtype=torch.float32).view([-1, 1])\n",
    "    y_true = fit_fn(x)\n",
    "    y_est = net.forward(x_pt).cpu().detach().numpy()\n",
    "\n",
    "    plt.plot(x, y_true, label=\"True\")\n",
    "    plt.plot(x, y_est, label=\"Estimate\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2ddbc99d-410b-4ca3-9a32-ea209f73de24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit_fn = lambda x : torch.ones_like(x) - torch.exp(-x)\n",
    "fit_fn = lambda x : -(x ** 2)\n",
    "a = -10\n",
    "b = 10\n",
    "dx = 0.1\n",
    "\n",
    "tparams = TParams(iters=int(5e3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "fa284246-1368-43ef-8214-785bf49509a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6z/xg9c2xxn1lg34383kjmcgxfw0000gn/T/ipykernel_40945/197256121.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x = torch.tensor(\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "loss() takes 2 positional arguments but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[59], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train(monotone_network, fit_fn, a, b, dx, tparams, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[57], line 28\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(net, fit_fn, a, b, dx, tparams, verbose)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m itr \u001b[38;5;129;01min\u001b[39;00m rng:\n\u001b[1;32m     27\u001b[0m     opt\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 28\u001b[0m     loss_this_itr \u001b[38;5;241m=\u001b[39m loss(net, x, y)\n\u001b[1;32m     30\u001b[0m     loss_this_itr\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (itr \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m):\n",
      "\u001b[0;31mTypeError\u001b[0m: loss() takes 2 positional arguments but 3 were given"
     ]
    }
   ],
   "source": [
    "train(monotone_network, fit_fn, a, b, dx, tparams, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54bef2d8-8046-4462-87b8-b24671abcea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(monotone_network, fit_fn, a, b, dx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84bc060a-99b1-4ae8-a35b-297622180894",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Submodular NN",
   "language": "python",
   "name": "submodular_nn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
