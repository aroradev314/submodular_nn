d vec:  [4 5 3 5 5]

First 5 training samples (X_train and corresponding rewards):
X_train[0] = [6 2 6 7 4], reward = 18
X_train[1] = [3 7 2 5 4], reward = 19
X_train[2] = [1 7 3 5 1], reward = 15
X_train[3] = [7 4 0 5 8], reward = 18
X_train[4] = [0 2 3 6 3], reward = 13

First 5 testing samples (X_test and corresponding rewards):
X_test[0] = [4 8 3 4 8], reward = 21
X_test[1] = [7 2 0 2 3], reward = 11
X_test[2] = [1 0 6 7 6], reward = 14
X_test[3] = [5 4 3 0 6], reward = 16
X_test[4] = [4 6 0 2 8], reward = 16
Epoch 1/100 - Train Loss: 210.8365, Test Loss: 228.7145
Epoch 2/100 - Train Loss: 184.2961, Test Loss: 202.2959
Epoch 3/100 - Train Loss: 161.2984, Test Loss: 178.1312
Epoch 4/100 - Train Loss: 140.7437, Test Loss: 156.5207
Epoch 5/100 - Train Loss: 122.4194, Test Loss: 137.4129
Epoch 6/100 - Train Loss: 106.2131, Test Loss: 120.0921
Epoch 7/100 - Train Loss: 91.8702, Test Loss: 105.0252
Epoch 8/100 - Train Loss: 79.3447, Test Loss: 91.3755
Epoch 9/100 - Train Loss: 68.3931, Test Loss: 79.5081
Epoch 10/100 - Train Loss: 58.9061, Test Loss: 69.2263
Epoch 11/100 - Train Loss: 50.7520, Test Loss: 60.0501
Epoch 12/100 - Train Loss: 43.7656, Test Loss: 52.2991
Epoch 13/100 - Train Loss: 37.8392, Test Loss: 45.5279
Epoch 14/100 - Train Loss: 32.8439, Test Loss: 39.7094
Epoch 15/100 - Train Loss: 28.6978, Test Loss: 34.5760
Epoch 16/100 - Train Loss: 25.2471, Test Loss: 30.6171
Epoch 17/100 - Train Loss: 22.4683, Test Loss: 27.0354
Epoch 18/100 - Train Loss: 20.1905, Test Loss: 24.1221
Epoch 19/100 - Train Loss: 18.3641, Test Loss: 21.6629
Epoch 20/100 - Train Loss: 16.9062, Test Loss: 19.6471
Epoch 21/100 - Train Loss: 15.7728, Test Loss: 18.0430
Epoch 22/100 - Train Loss: 14.9025, Test Loss: 16.6024
Epoch 23/100 - Train Loss: 14.2378, Test Loss: 15.5740
Epoch 24/100 - Train Loss: 13.7297, Test Loss: 14.6364
Epoch 25/100 - Train Loss: 13.3721, Test Loss: 13.8311
Epoch 26/100 - Train Loss: 13.0886, Test Loss: 13.2343
Epoch 27/100 - Train Loss: 12.9067, Test Loss: 12.7579
Epoch 28/100 - Train Loss: 12.7532, Test Loss: 12.4250
Epoch 29/100 - Train Loss: 12.6447, Test Loss: 12.0756
Epoch 30/100 - Train Loss: 12.5645, Test Loss: 11.8992
Epoch 31/100 - Train Loss: 12.5186, Test Loss: 11.6287
Epoch 32/100 - Train Loss: 12.4999, Test Loss: 11.5007
Epoch 33/100 - Train Loss: 12.4853, Test Loss: 11.3532
Epoch 34/100 - Train Loss: 12.4744, Test Loss: 11.2586
Epoch 35/100 - Train Loss: 12.4415, Test Loss: 11.2287
Epoch 36/100 - Train Loss: 12.4335, Test Loss: 11.1147
Epoch 37/100 - Train Loss: 12.4276, Test Loss: 11.1080
Epoch 38/100 - Train Loss: 12.4506, Test Loss: 10.9871
Epoch 39/100 - Train Loss: 12.4527, Test Loss: 11.0308
Epoch 40/100 - Train Loss: 12.4359, Test Loss: 11.0511
Traceback (most recent call last):
  File "/Users/devarora/ml/submodular_nn/submodular_net.py", line 99, in <module>
    optimizer.step()
  File "/opt/homebrew/Caskroom/miniconda/base/envs/submodular_nn/lib/python3.12/site-packages/torch/optim/optimizer.py", line 391, in wrapper
    out = func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^
  File "/opt/homebrew/Caskroom/miniconda/base/envs/submodular_nn/lib/python3.12/site-packages/torch/optim/optimizer.py", line 76, in _use_grad
    ret = func(self, *args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/homebrew/Caskroom/miniconda/base/envs/submodular_nn/lib/python3.12/site-packages/torch/optim/adam.py", line 168, in step
    adam(
  File "/opt/homebrew/Caskroom/miniconda/base/envs/submodular_nn/lib/python3.12/site-packages/torch/optim/adam.py", line 318, in adam
    func(params,
  File "/opt/homebrew/Caskroom/miniconda/base/envs/submodular_nn/lib/python3.12/site-packages/torch/optim/adam.py", line 441, in _single_tensor_adam
    denom = (exp_avg_sq.sqrt() / bias_correction2_sqrt).add_(eps)
             ^^^^^^^^^^^^^^^^^
KeyboardInterrupt
