d vec:  [4 5 3 5 5]

First 5 training samples (X_train and corresponding rewards):
X_train[0] = [6 2 6 7 4], reward = 25
X_train[1] = [3 7 2 5 4], reward = 21
X_train[2] = [1 7 3 5 1], reward = 17
X_train[3] = [7 4 0 5 8], reward = 24
X_train[4] = [0 2 3 6 3], reward = 14

First 5 testing samples (X_test and corresponding rewards):
X_test[0] = [4 8 3 4 8], reward = 27
X_test[1] = [7 2 0 2 3], reward = 14
X_test[2] = [1 0 6 7 6], reward = 20
X_test[3] = [5 4 3 0 6], reward = 18
X_test[4] = [4 6 0 2 8], reward = 20
Epoch 1/100 - Train Loss: 342.2314, Test Loss: 372.7454
Epoch 2/100 - Train Loss: 309.1779, Test Loss: 339.7748
Epoch 3/100 - Train Loss: 279.7759, Test Loss: 308.2281
Epoch 4/100 - Train Loss: 252.5413, Test Loss: 279.7568
Epoch 5/100 - Train Loss: 227.5775, Test Loss: 253.3446
Epoch 6/100 - Train Loss: 204.8715, Test Loss: 229.1308
Epoch 7/100 - Train Loss: 184.2022, Test Loss: 206.7574
Epoch 8/100 - Train Loss: 165.4014, Test Loss: 186.3667
Epoch 9/100 - Train Loss: 148.4067, Test Loss: 167.7666
Epoch 10/100 - Train Loss: 133.0995, Test Loss: 151.2110
Epoch 11/100 - Train Loss: 119.3533, Test Loss: 136.1919
Epoch 12/100 - Train Loss: 107.0857, Test Loss: 122.3002
Epoch 13/100 - Train Loss: 96.1035, Test Loss: 109.9740
Epoch 14/100 - Train Loss: 86.3940, Test Loss: 98.8233
Epoch 15/100 - Train Loss: 77.7717, Test Loss: 89.1136
Epoch 16/100 - Train Loss: 70.2426, Test Loss: 80.4326
Epoch 17/100 - Train Loss: 63.6267, Test Loss: 72.4315
Epoch 18/100 - Train Loss: 57.8593, Test Loss: 65.4507
Epoch 19/100 - Train Loss: 52.8724, Test Loss: 59.4359
Epoch 20/100 - Train Loss: 48.5800, Test Loss: 54.4230
Epoch 21/100 - Train Loss: 44.9625, Test Loss: 49.5048
Epoch 22/100 - Train Loss: 41.8238, Test Loss: 45.6206
Epoch 23/100 - Train Loss: 39.1998, Test Loss: 41.8737
Epoch 24/100 - Train Loss: 37.0450, Test Loss: 38.8193
Epoch 25/100 - Train Loss: 35.2123, Test Loss: 36.4023
Epoch 26/100 - Train Loss: 33.7505, Test Loss: 34.0139
Epoch 27/100 - Train Loss: 32.5031, Test Loss: 32.2326
Epoch 28/100 - Train Loss: 31.5692, Test Loss: 30.3455
Epoch 29/100 - Train Loss: 30.7452, Test Loss: 29.0967
Epoch 30/100 - Train Loss: 30.1065, Test Loss: 28.0609
Epoch 31/100 - Train Loss: 29.6161, Test Loss: 26.8995
Epoch 32/100 - Train Loss: 29.2507, Test Loss: 26.0393
Epoch 33/100 - Train Loss: 28.9097, Test Loss: 25.3711
Epoch 34/100 - Train Loss: 28.6985, Test Loss: 24.8038
Epoch 35/100 - Train Loss: 28.5148, Test Loss: 24.3831
Epoch 36/100 - Train Loss: 28.3844, Test Loss: 23.9462
Epoch 37/100 - Train Loss: 28.2506, Test Loss: 23.6172
Epoch 38/100 - Train Loss: 28.1669, Test Loss: 23.3148
Epoch 39/100 - Train Loss: 28.1121, Test Loss: 23.0921
Epoch 40/100 - Train Loss: 28.0685, Test Loss: 22.7466
Epoch 41/100 - Train Loss: 28.0232, Test Loss: 22.6207
Epoch 42/100 - Train Loss: 28.0015, Test Loss: 22.4556
Epoch 43/100 - Train Loss: 27.9768, Test Loss: 22.3857
Epoch 44/100 - Train Loss: 27.9611, Test Loss: 22.3390
Epoch 45/100 - Train Loss: 27.9882, Test Loss: 22.1720
Epoch 46/100 - Train Loss: 27.9657, Test Loss: 22.0575
Epoch 47/100 - Train Loss: 27.9494, Test Loss: 22.1122
Epoch 48/100 - Train Loss: 27.9530, Test Loss: 21.9868
Epoch 49/100 - Train Loss: 27.9462, Test Loss: 21.9417
Epoch 50/100 - Train Loss: 27.9382, Test Loss: 21.8765
Epoch 51/100 - Train Loss: 27.9351, Test Loss: 21.8411
Epoch 52/100 - Train Loss: 27.9271, Test Loss: 21.7796
Epoch 53/100 - Train Loss: 27.9809, Test Loss: 21.7832
Epoch 54/100 - Train Loss: 27.9330, Test Loss: 21.8663
Epoch 55/100 - Train Loss: 27.9330, Test Loss: 21.8143
Epoch 56/100 - Train Loss: 27.9647, Test Loss: 21.8211
Epoch 57/100 - Train Loss: 27.9624, Test Loss: 21.8668
Epoch 58/100 - Train Loss: 27.9299, Test Loss: 21.6930
Epoch 59/100 - Train Loss: 27.9374, Test Loss: 21.7436
Epoch 60/100 - Train Loss: 27.9428, Test Loss: 21.7176
Epoch 61/100 - Train Loss: 27.9598, Test Loss: 21.6989
Epoch 62/100 - Train Loss: 27.9615, Test Loss: 21.7914
Epoch 63/100 - Train Loss: 27.9410, Test Loss: 21.7505
Epoch 64/100 - Train Loss: 27.9341, Test Loss: 21.7024
Epoch 65/100 - Train Loss: 27.9326, Test Loss: 21.6881
Epoch 66/100 - Train Loss: 27.9597, Test Loss: 21.7017
Epoch 67/100 - Train Loss: 27.9335, Test Loss: 21.7678
Epoch 68/100 - Train Loss: 27.9432, Test Loss: 21.7442
Epoch 69/100 - Train Loss: 27.9465, Test Loss: 21.7092
Epoch 70/100 - Train Loss: 27.9463, Test Loss: 21.6685
Epoch 71/100 - Train Loss: 27.9321, Test Loss: 21.6502
Epoch 72/100 - Train Loss: 27.9707, Test Loss: 21.7210
Epoch 73/100 - Train Loss: 27.9377, Test Loss: 21.6712
Epoch 74/100 - Train Loss: 27.9303, Test Loss: 21.6871
Epoch 75/100 - Train Loss: 27.9512, Test Loss: 21.7489
Epoch 76/100 - Train Loss: 27.9566, Test Loss: 21.6731
Epoch 77/100 - Train Loss: 27.9286, Test Loss: 21.6860
Epoch 78/100 - Train Loss: 27.9367, Test Loss: 21.7174
Epoch 79/100 - Train Loss: 27.9666, Test Loss: 21.6302
Epoch 80/100 - Train Loss: 27.9371, Test Loss: 21.7205
Epoch 81/100 - Train Loss: 27.9518, Test Loss: 21.7799
Epoch 82/100 - Train Loss: 27.9291, Test Loss: 21.6588
Epoch 83/100 - Train Loss: 27.9473, Test Loss: 21.6604
Epoch 84/100 - Train Loss: 27.9315, Test Loss: 21.7478
Epoch 85/100 - Train Loss: 27.9435, Test Loss: 21.8053
Epoch 86/100 - Train Loss: 28.0090, Test Loss: 21.6472
Epoch 87/100 - Train Loss: 27.9818, Test Loss: 21.6572
Epoch 88/100 - Train Loss: 27.9735, Test Loss: 21.7397
Epoch 89/100 - Train Loss: 27.9569, Test Loss: 21.7509
Epoch 90/100 - Train Loss: 27.9380, Test Loss: 21.7179
Epoch 91/100 - Train Loss: 27.9637, Test Loss: 21.7347
Epoch 92/100 - Train Loss: 27.9324, Test Loss: 21.6461
Epoch 93/100 - Train Loss: 27.9499, Test Loss: 21.6976
Epoch 94/100 - Train Loss: 27.9459, Test Loss: 21.6715
Epoch 95/100 - Train Loss: 27.9425, Test Loss: 21.7240
Epoch 96/100 - Train Loss: 27.9598, Test Loss: 21.6561
Epoch 97/100 - Train Loss: 27.9309, Test Loss: 21.6249
Epoch 98/100 - Train Loss: 27.9750, Test Loss: 21.7151
Epoch 99/100 - Train Loss: 27.9660, Test Loss: 21.8047
Traceback (most recent call last):
  File "/Users/devarora/ml/submodular_nn/submodular_net.py", line 98, in <module>
    optimizer.step()
  File "/opt/homebrew/Caskroom/miniconda/base/envs/submodular_nn/lib/python3.12/site-packages/torch/optim/optimizer.py", line 391, in wrapper
    out = func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^
  File "/opt/homebrew/Caskroom/miniconda/base/envs/submodular_nn/lib/python3.12/site-packages/torch/optim/optimizer.py", line 76, in _use_grad
    ret = func(self, *args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/homebrew/Caskroom/miniconda/base/envs/submodular_nn/lib/python3.12/site-packages/torch/optim/adam.py", line 168, in step
    adam(
  File "/opt/homebrew/Caskroom/miniconda/base/envs/submodular_nn/lib/python3.12/site-packages/torch/optim/adam.py", line 318, in adam
    func(params,
  File "/opt/homebrew/Caskroom/miniconda/base/envs/submodular_nn/lib/python3.12/site-packages/torch/optim/adam.py", line 432, in _single_tensor_adam
    bias_correction2_sqrt = _dispatch_sqrt(bias_correction2)
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/homebrew/Caskroom/miniconda/base/envs/submodular_nn/lib/python3.12/site-packages/torch/optim/optimizer.py", line 97, in _dispatch_sqrt
    def _dispatch_sqrt(x: float):  # float annotation is needed because of torchscript type inference

KeyboardInterrupt
